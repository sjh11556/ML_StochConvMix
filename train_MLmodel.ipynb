{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import xarray as xr\n",
    "#import kerastuner as kt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training dw/dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def new_sample(df,nn):\n",
    "    dlen=len(df)\n",
    "    if nn<dlen:\n",
    "        return df.sample(n=nn)\n",
    "    else:\n",
    "        return df.sample(frac=1)\n",
    "    \n",
    "w_bin=[0,1,2,3,4,1000]\n",
    "\n",
    "var_select=['B','rl','w','t_p','rt_p','thv_env_grad','ent_c_pre','dwdt_pre','ent_t_pre','det_t_pre','ent_c','dwdt','ent_t','det_t']\n",
    "model_index=1\n",
    "data_dir=\"../bomex/data\"\n",
    "dataset_bomex=xr.open_dataset(data_dir+\"/\"+\"sample_clouds_bomex.nc\").isel(time=slice(1,119)).to_dataframe().reset_index()\n",
    "data_dir=\"../rico/data\"\n",
    "dataset_rico=xr.open_dataset(data_dir+\"/\"+\"sample_clouds_rico.nc\").isel(time=slice(1,239)).to_dataframe().reset_index()\n",
    "\n",
    "dataset=pd.concat([\n",
    "                   dataset_bomex[(dataset_bomex['zt']>25) & (dataset_bomex['ent_c']>0) & (dataset_bomex['ent_c_pre']>0) & (dataset_bomex['ent_t']>0) & (dataset_bomex['ent_t_pre']>0) & (dataset_bomex['det_t']>0) & (dataset_bomex['det_t_pre']>0) ][var_select].dropna(),\n",
    "                   dataset_rico[(dataset_rico['zt']>40)   & (dataset_rico['ent_c']>0)  & (dataset_rico['ent_c_pre']>0)  & (dataset_rico['ent_t']>0)  & (dataset_rico['ent_t_pre']>0)  & (dataset_rico['det_t']>0)  & (dataset_rico['det_t_pre']>0)  ][var_select].dropna()\n",
    "])\n",
    "\n",
    "dataset['ent_c']=np.log(dataset['ent_c'])\n",
    "dataset['ent_c_pre']=np.log(dataset['ent_c_pre'])\n",
    "\n",
    "dataset['ent_t']=np.log(dataset['ent_t'])\n",
    "dataset['ent_t_pre']=np.log(dataset['ent_t_pre'])\n",
    "dataset['det_t']=np.log(dataset['det_t'])\n",
    "dataset['det_t_pre']=np.log(dataset['det_t_pre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_var=['ent_c','dwdt','ent_t','det_t']\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "\n",
    "train_stats = train_dataset.describe(percentiles=[.001,.05, .95,.999])\n",
    "train_stats['ent_c_pre']=train_stats['ent_c']\n",
    "train_stats['dwdt_pre']=train_stats['dwdt']\n",
    "train_stats['ent_t_pre']=train_stats['ent_t']\n",
    "train_stats['det_t_pre']=train_stats['det_t']\n",
    "train_stats = train_stats.transpose()\n",
    "\n",
    "def norm(x,varlist):\n",
    "    y=x.copy()\n",
    "    y[varlist]=(y[varlist]) / train_stats['std'][varlist]\n",
    "    return y\n",
    "\n",
    "train_dataset = norm(train_dataset,var_select)\n",
    "test_dataset = norm(test_dataset,var_select)\n",
    "\n",
    "train_stats.to_csv(\"saved_model/model_stat_%d.csv\" % model_index)\n",
    "\n",
    "\n",
    "train_labels  = train_dataset[out_var].copy()\n",
    "train_dataset = train_dataset.drop(out_var, axis=1)\n",
    "test_labels   = test_dataset[out_var].copy()\n",
    "test_dataset  = test_dataset.drop(out_var, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "negloglik = [lambda y1, p_y1:  -p_y1.log_prob(y1), lambda y2, p_y2:  -p_y2.log_prob(y2) , lambda y3, p_y3:  -p_y3.log_prob(y3), lambda y4, p_y4:  -p_y4.log_prob(y4)]\n",
    "\n",
    "def build_model():\n",
    "    inputs = tf.keras.Input(shape=[len(train_dataset.keys())])\n",
    "    x = tf.keras.layers.Dense(units=16, activation='selu')(inputs[...,:6])\n",
    "    x = tf.keras.layers.Dense(16, activation='selu')(x)\n",
    "    x = tf.keras.layers.Dense(16, activation='selu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(12)(x)\n",
    "\n",
    "    \n",
    "    loc1   = tf.math.softplus(0.1 *x[..., 0:1])*(x[..., 1:2]-inputs[...,6:7])*60.0 + inputs[...,6:7]\n",
    "    scale1 = tf.math.softplus(0.1 *x[..., 2:3])*tf.sqrt(60.0)\n",
    "    \n",
    "    loc2   = tf.math.softplus(0.1 *x[..., 3:4])*(x[..., 4:5]-inputs[...,7:8])*60.0 + inputs[...,7:8]\n",
    "    scale2 = tf.math.softplus(0.1 *x[..., 5:6])*tf.sqrt(60.0)\n",
    "    \n",
    "    loc3   = tf.math.softplus(0.1 *x[..., 6:7])*(x[..., 7:8]-inputs[...,8:9])*60.0 + inputs[...,8:9]\n",
    "    scale3 = tf.math.softplus(0.1 *x[..., 8:9])*tf.sqrt(60.0)\n",
    "    \n",
    "    loc4   = tf.math.softplus(0.1 *x[..., 9:10])*(x[..., 10:11]-inputs[...,9:10])*60.0 + inputs[...,9:10]\n",
    "    scale4 = tf.math.softplus(0.1 *x[..., 11:12])*tf.sqrt(60.0)\n",
    "    \n",
    "    \n",
    "    scale1=tf.clip_by_value(scale1,clip_value_min=5e-3,clip_value_max=10.0)\n",
    "    scale2=tf.clip_by_value(scale2,clip_value_min=5e-3,clip_value_max=10.0)\n",
    "    scale3=tf.clip_by_value(scale3,clip_value_min=5e-3,clip_value_max=10.0)\n",
    "    scale4=tf.clip_by_value(scale4,clip_value_min=5e-3,clip_value_max=10.0)\n",
    "    \n",
    "    x1=tf.concat([loc1,scale1],axis=-1)\n",
    "    x2=tf.concat([loc2,scale2],axis=-1)\n",
    "    x3=tf.concat([loc3,scale3],axis=-1)\n",
    "    x4=tf.concat([loc4,scale4],axis=-1)\n",
    "    \n",
    "    outputs1 = tfp.layers.DistributionLambda(\n",
    "        lambda t: tfd.Normal(loc=t[...,:1] ,scale=t[..., 1:]   ))(x1)\n",
    "    outputs2 = tfp.layers.DistributionLambda(\n",
    "        lambda t: tfd.Normal(loc=t[...,:1] ,scale=t[..., 1:]   ))(x2)\n",
    "    outputs3 = tfp.layers.DistributionLambda(\n",
    "        lambda t: tfd.Normal(loc=t[...,:1] ,scale=t[..., 1:]   ))(x3)\n",
    "    \n",
    "    outputs4 = tfp.layers.DistributionLambda(\n",
    "        lambda t: tfd.Normal(loc=t[...,:1] ,scale=t[..., 1:]   ))(x4)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[outputs1,outputs2,outputs3,outputs4], name='tiny_model')\n",
    "    \n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss=negloglik)\n",
    "    return model\n",
    "\n",
    "    \n",
    "model = build_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, [train_labels['ent_c'],train_labels['dwdt'],train_labels['ent_t'],train_labels['det_t']],\n",
    "    epochs=EPOCHS, callbacks=[early_stop], validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp=plt.hist(np.array(test_predictions[:,2]),bins=100)\n",
    "#tmp=plt.hist2d(np.array(test_predictions[:,1]),np.array(test_predictions[:,2]),bins=40)\n",
    "\n",
    "\n",
    "model.save(\"saved_model/model_%d.h5\" % model_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cdat81] *",
   "language": "python",
   "name": "conda-env-cdat81-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
